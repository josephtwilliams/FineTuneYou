{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start here\n",
    "1. Upload your resume into the root directory and remove `resume_example.pdf`\n",
    "2. Fill out the additional questions in the `questions.json` file (Feel free to add your own)\n",
    "3. Work through each cell in this document (`FineTuneYou.py`)\n",
    "\n",
    "**Note:** If any errors occur, please ensure that the correct modules are installed, and your python environment is properly configured.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import sys\n",
    "sys.path.append(\"src\")\n",
    "from data_upload import create_text_from_pdf, create_text_from_questions\n",
    "from questions_creation import generate_resume_questions\n",
    "from fine_tune import collate_fine_tune_data, split_data, upload_train_test_files, create_finetune_job, check_job\n",
    "from embedding import create_embeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert resume.pdf and questions.json to text files and store them in './src/text_files'\n",
    "create_text_from_pdf()\n",
    "create_text_from_questions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate question/answer pairs based on the uploaded resume using gpt-3.5-turbo\n",
    "generate_resume_questions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize the data needed for fine-tuning and evaluation\n",
    "collate_fine_tune_data()\n",
    "split_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the training and validation files and get their IDs\n",
    "training_file_id, validation_file_id = upload_train_test_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the fine-tuning process\n",
    "suffix = \"FineTuneYou\" #Choose a suffix for your fine-tuned model\n",
    "job_id = create_finetune_job(training_file_id, validation_file_id, suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the status of the fine-tuning job\n",
    "# The fine-tuning process usually takes around 5-10 Minutes\n",
    "check_job(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings\n",
    "create_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the ID of the fine-tuned model\n",
    "# If \"Fine-tuned model ID: None\" Please wait for the fine-tuning to finish.\n",
    "response = openai.FineTuningJob.retrieve(job_id)\n",
    "fine_tuned_model_id = response[\"fine_tuned_model\"]\n",
    "\n",
    "print(\"Fine-tuned model ID:\", fine_tuned_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the language model with the fine-tuned model ID\n",
    "llm_name=fine_tuned_model_id\n",
    "persist_directory = 'docs/chroma/'\n",
    "embedding = OpenAIEmbeddings()\n",
    "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)\n",
    "llm = ChatOpenAI(model_name=llm_name, temperature=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt template for the QA chain\n",
    "template = \"\"\"Your task is to accurately represent professional and educational background, as well as interests and hobbies, while speaking in the first person. Kindly refrain from answering questions that aren't related to these topics.\n",
    "{context}\n",
    "Question: How long have you worked as an Amazon Associate?\n",
    "Answer: I have worked as an Amazon Associate for five years.\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "\n",
    "# Run the QA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the QA chain with a sample query\n",
    "question = \"Where did you grow up?\"\n",
    "result = qa_chain({\"query\": question})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESET ENVIRONMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "def reset_project():\n",
    "    # Remove specific text files in ./src/text_files/ except inappropriate_questions.txt\n",
    "    text_files = glob.glob('./src/text_files/*.txt')\n",
    "    for file in text_files:\n",
    "        if 'inappropriate_questions.txt' not in file:\n",
    "            os.remove(file)\n",
    "    \n",
    "    # Remove specific .json and .jsonl files in ./src/ \n",
    "    json_files = glob.glob('./src/*.json')\n",
    "    jsonl_files = glob.glob('./src/*.jsonl')\n",
    "    for file in json_files + jsonl_files:\n",
    "        os.remove(file)\n",
    "            \n",
    "    # Remove the entire ./docs/ directory\n",
    "    if os.path.exists('./docs'):\n",
    "        shutil.rmtree('./docs')\n",
    "    \n",
    "    print(\"Project reset completed. All selected generated files have been removed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to reset the project\n",
    "reset_project()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
